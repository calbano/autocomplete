source('autocomplete.R')
source('randomizedSamples.R')
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(microbenchmark))
# How performances Scales With Number of Words
# Uses random samples from movies file
random_sample_sizes <- function(n_times = 5) {
# Run microbenchmark n_times (default is 5)
file <- movies
sample_sizes <- numeric(75)
for (i in 1:75) {
values <- c(seq(0, 6, 0.5)[-1], seq(6, 12.3, 0.1)[-1])
sample_sizes[i] <- exp(values[i])
}
time_at_samplesize <- vector(length = length(sample_sizes))
# Mean run time over n_times evaluations
for (i in 1:length(sample_sizes)) {
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
time_at_samplesize[i] <-
mean(microbenchmark(occurrences(query, sorted_movies), times = n_times)$time)
}
# Convert nanoseconds to Seconds
time_at_samplesize_seconds <- time_at_samplesize * 1e-9
#fit <- lm(time_at_samplesize ~ log(sample_sizes))
options(device = "png")
png("performance2.png")
plot(sample_sizes, time_at_samplesize_seconds, xlab = "Number of Words", ylab = "Time in Seconds",
main = "Performance With Number of Words")
#curve(coef(fit)[1] + coef(fit)[2] * log(x), add = TRUE, col = 'red')
curve(log(x), add=TRUE, col = 'blue')
dev.off()
#return(c(coef(fit)[1], coef(fit)[2]))
}
random_sample_sizes()
suppressPackageStartupMessages(library(assertthat))
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(bit64))
# assumes file is data folder within working directory
read_terms <- function(file_name) {
file_path <- paste("./data", file_name, sep = "/")
terms_df <- fread(file_path, skip = 1, header=FALSE)
names(terms_df) <- c("weight", "term")
# Check that correct number of terms were imported (as specified by first line)
total_terms <- read.table(file_path, nrows = 1, header=FALSE)
assert_that(nrow(terms_df) == as.numeric(total_terms))
# Change words to lower case
terms_df$term <- tolower(terms_df$term)
# Remove punctuation
terms_df$term <- gsub('[[:punct:]]', "", terms_df$term)
return(terms_df)
}
sort_input<- function(values) {
# Sorts vectors & data frames
if (is.vector(values)) {
values_sorted <- sort(values)
assert_that(!any(is.unsorted(values_sorted)))
} else if (is.data.frame(values)) {
values_sorted <- values[sort(values$term, index.return=TRUE)$ix,]
assert_that(!any(is.unsorted(values_sorted$term)))
}
return(values_sorted)
}
first_occurrence <- function(query, search_values) {
# Takes Query Term and Sorted Vector of Values to Search Through
assert_that(is.vector(search_values))
assert_that(!any(is.unsorted(search_values)))
first <- 0
last <- length(search_values)
diff <- last - first
# Assert that each element between [first, last) exists
assert_that(!any(is.na(search_values)))
while (diff != 1) {
rounded_mid <- ceiling((first + last) / 2)
if (query <= search_values[rounded_mid]) {
first <- first
last <- rounded_mid
} else if (query > search_values[rounded_mid]) {
first <- rounded_mid
last <- last
}
diff <- last - first
}
# Sets the location/index at which to check for query
if (first == 0 |
(query > search_values[first] && query <= search_values[last])) {
location = last
} else if (query > search_values[first] && query > search_values[last]) {
return(NA)
}
# Checks whether the query is found at the index
prefix_pattern <- paste("^", query, sep = "", collapse="")
prefix_found <- grep(prefix_pattern, search_values[location])
if (length(prefix_found) != 0) {
return(location)
} else {
return(NA)
}
}
occurrences <- function(query, search_values) {
assert_that(is.vector(search_values))
assert_that(!any(is.unsorted(search_values)))
first_occ <- first_occurrence(query, search_values)
# If first occurrence is found, initialize search to be (first, last]
if (!is.na(first_occ)) {
first <- first_occ
last <- length(search_values) + 1
} else {
return(NA)
}
diff <- last - first
# Define a match pattern
match <- paste("^", query, sep = "", collapse="")
while (diff != 1) {
rounded_mid <- ceiling((first + last) / 2)
# outputs 0 if match is not found and 1 if match is found
match_found <-  as.numeric(length(grep(match, search_values[rounded_mid]) != 0))
if (match_found != 0) {
first <- rounded_mid
last <- last
} else {
first <- first
last <- rounded_mid
}
diff <- last - first
}
# Check if there is a positive match at these indices
match_at_first <- length(grep(match, search_values[first]))
# If index for last is greater than length of vector
if (last > length(search_values)) {
match_at_last <- 0
} else {
match_at_last <- length(grep(match, search_values[last]))
}
if (match_at_first != 0 && match_at_last == 0) {
last_occ <- first
}
assert_that(match_at_first != 0 && match_at_last == 0)
# Returns vector in form (index of first occurrence, index last occurrence)
return(c(first_occ, last_occ))
}
autocomplete <- function(query, search_df, n) {
# Combines steps
sorted_data <- sort_input(search_df)
# Returns pair of lower and upper indices
pair_indices <- occurrences(query, sorted_data$term)
subset_by_query <- sorted_data[(pair_indices[1]:pair_indices[2]),]
# Ordered by term weight (in decreasing order)
order_by_weight <- subset_by_query[order(-subset_by_query$weight),]
if (n >= nrow(order_by_weight)) {
return(order_by_weight)
} else {
return(order_by_weight[(1:n),])
}
}
source('autocomplete.R')
source('randomizedSamples.R')
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(microbenchmark))
# How performances Scales With Number of Words
# Uses random samples from movies file
random_sample_sizes <- function(n_times = 5) {
# Run microbenchmark n_times (default is 5)
file <- movies
sample_sizes <- numeric(75)
for (i in 1:75) {
values <- c(seq(0, 6, 0.5)[-1], seq(6, 12.3, 0.1)[-1])
sample_sizes[i] <- exp(values[i])
}
time_at_samplesize <- vector(length = length(sample_sizes))
# Mean run time over n_times evaluations
for (i in 1:length(sample_sizes)) {
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
time_at_samplesize[i] <-
mean(microbenchmark(occurrences(query, sorted_movies), times = n_times)$time)
}
# Convert nanoseconds to Seconds
time_at_samplesize_seconds <- time_at_samplesize * 1e-9
#fit <- lm(time_at_samplesize ~ log(sample_sizes))
options(device = "png")
png("performance2.png")
plot(sample_sizes, time_at_samplesize_seconds, xlab = "Number of Words", ylab = "Time in Seconds",
main = "Performance With Number of Words")
#curve(coef(fit)[1] + coef(fit)[2] * log(x), add = TRUE, col = 'red')
dev.off()
#return(c(coef(fit)[1], coef(fit)[2]))
}
random_sample_sizes()
query
microbenchmark(occurrences(query, sorted_movies), times = n_times)$time
?is.unsorted
is.unsorted()
is.unsorted
source('~/Documents/autocomplete/autocomplete.R')
random_sample_sizes()
microbenchmark(occurrences("the", sorted_movies))
seq(0, 6, 0.5)
values
exp(values)
round(values, 2)
round(exp(values,2))
round(exp(values),2)
values <- c(1:6, seq(6, 12.3, 0.1)[-1])
values
exp(values)
source('autocomplete.R')
source('randomizedSamples.R')
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(microbenchmark))
# How performances Scales With Number of Words
# Uses random samples from movies file
random_sample_sizes <- function(n_times = 5) {
# Run microbenchmark n_times (default is 5)
file <- movies
sample_sizes <- numeric(75)
for (i in 1:75) {
values <- c(1:6, seq(6, 12.3, 0.1)[-1])
sample_sizes[i] <- exp(values[i])
}
time_at_samplesize <- vector(length = length(sample_sizes))
# Mean run time over n_times evaluations
for (i in 1:length(sample_sizes)) {
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
time_at_samplesize[i] <-
mean(microbenchmark(occurrences(query, sorted_movies), times = n_times)$time)
}
# Convert nanoseconds to Seconds
time_at_samplesize_seconds <- time_at_samplesize * 1e-9
fit <- lm(time_at_samplesize_seconds ~ log(sample_sizes))
options(device = "png")
png("performance2.png")
plot(sample_sizes, time_at_samplesize_seconds, xlab = "Number of Words", ylab = "Time in Seconds",
main = "Performance With Number of Words")
curve(coef(fit)[1] + coef(fit)[2] * log(x), add = TRUE, col = 'red')
dev.off()
#return(c(coef(fit)[1], coef(fit)[2]))
}
random_sample_sizes()
source('~/Documents/autocomplete/autocomplete.R')
source('~/Documents/autocomplete/randomizedSamples.R')
source('autocomplete.R')
source('randomizedSamples.R')
random_sample_sizes <- function(n_times = 5) {
# Run microbenchmark n_times (default is 5)
file <- movies
sample_sizes <- numeric(75)
for (i in 1:75) {
values <- c(1:6, seq(6, 12.3, 0.1)[-1])
sample_sizes[i] <- exp(values[i])
}
time_at_samplesize <- vector(length = length(sample_sizes))
# Mean run time over n_times evaluations
for (i in 1:length(sample_sizes)) {
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
time_at_samplesize[i] <-
mean(microbenchmark(occurrences(query, sorted_movies), times = n_times)$time)
}
# Convert nanoseconds to Seconds
time_at_samplesize_seconds <- time_at_samplesize * 1e-9
fit <- lm(time_at_samplesize_seconds ~ log(sample_sizes))
options(device = "png")
png("performance2.png")
plot(sample_sizes, time_at_samplesize_seconds, xlab = "Number of Words", ylab = "Time in Seconds",
main = "Performance With Number of Words")
curve(coef(fit)[1] + coef(fit)[2] * log(x), add = TRUE, col = 'red')
dev.off()
#return(c(coef(fit)[1], coef(fit)[2]))
}
random_sample_sizes()
file <- movies
sample_sizes <- numeric(75)
for (i in 1:75) {
values <- c(1:6, seq(6, 12.3, 0.1)[-1])
sample_sizes[i] <- exp(values[i])
}
time_at_samplesize <- vector(length = length(sample_sizes))
sorted_movies <- sort_input(sample_of_movies$term)
for (i in 1:length(sample_sizes)) {
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
time_at_samplesize[i] <-
mean(microbenchmark(occurrences(query, sorted_movies), times = n_times)$time)
}
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
?sample_n
file
suppressPackageStartupMessages(library(dplyr))
sample_of_movies <- sample_n(file, sample_sizes[i])
sample_n(movies, 2)
sample_sizes
length(values)
source('autocomplete.R')
source('randomizedSamples.R')
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(microbenchmark))
# How performances Scales With Number of Words
# Uses random samples from movies file
random_sample_sizes <- function(n_times = 5) {
# Run microbenchmark n_times (default is 5)
file <- movies
sample_sizes <- numeric(69)
for (i in 1:69) {
values <- c(1:6, seq(6, 12.3, 0.1)[-1])
sample_sizes[i] <- exp(values[i])
}
time_at_samplesize <- vector(length = length(sample_sizes))
# Mean run time over n_times evaluations
for (i in 1:length(sample_sizes)) {
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
time_at_samplesize[i] <-
mean(microbenchmark(occurrences(query, sorted_movies), times = n_times)$time)
}
# Convert nanoseconds to Seconds
time_at_samplesize_seconds <- time_at_samplesize * 1e-9
fit <- lm(time_at_samplesize_seconds ~ log(sample_sizes))
options(device = "png")
png("performance2.png")
plot(sample_sizes, time_at_samplesize_seconds, xlab = "Number of Words", ylab = "Time in Seconds",
main = "Performance With Number of Words")
curve(coef(fit)[1] + coef(fit)[2] * log(x), add = TRUE, col = 'red')
dev.off()
#return(c(coef(fit)[1], coef(fit)[2]))
}
random_sample_sizes()
time_at_samplesize_seconds[1]
coef(fit)[1] + coef(fit)[2] * log(x)
coef(fit)[1] + coef(fit)[2] * log( 0.0003466402)
coef(fit)[1] + coef(fit)[2] * log( 0.0003466402) * 10e-9
coef(fit)[1] + coef(fit)[2] * log( 0.0003466402) * 1e-9
318202.1 *1e-9
source('autocomplete.R')
source('randomizedSamples.R')
# Sort Before Running Binary Search
sorted_movies <- sort_input(movies$term)
microbenchmark(linear_search("the", sorted_movies))
microbenchmark(occurrences("the", sorted_movies))
source('autocomplete.R')
source('randomizedSamples.R')
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(microbenchmark))
# How performances Scales With Number of Words
# Uses random samples from movies file
random_sample_sizes <- function(n_times = 5) {
# Evaluate expression using microbenchmark n_times (default is 5)
file <- movies
sample_sizes <- numeric(69)
for (i in 1:69) {
values <- c(1:6, seq(6, 12.3, 0.1)[-1])
sample_sizes[i] <- exp(values[i])
}
time_at_samplesize <- vector(length = length(sample_sizes))
# Mean run time over n_times evaluations
for (i in 1:length(sample_sizes)) {
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
time_at_samplesize[i] <-
mean(microbenchmark(occurrences(query, sorted_movies), times = n_times)$time)
}
# Convert nanoseconds to Seconds
time_at_samplesize_seconds <- time_at_samplesize * 1e-9
fit <- lm(time_at_samplesize_seconds ~ log(sample_sizes))
options(device = "png")
png("performance.png")
plot(sample_sizes, time_at_samplesize_seconds, xlab = "Number of Words", ylab = "Time in Seconds",
main = "Performance With Number of Words")
curve(coef(fit)[1] + coef(fit)[2] * log(x), add = TRUE, col = 'red')
dev.off()
return(c(coef(fit)[1], coef(fit)[2]))
}
microbenchmark(linear_search("an", sorted_movies))
microbenchmark(occurrences("an", sorted_movies))
microbenchmark(linear_search("un", sorted_movies))
source('autocomplete.R')
suppressPackageStartupMessages(library(dplyr))
movies <- read_terms('movies.txt')
babyNames <- read_terms("baby-names.txt")
pokemon <- read_terms("pokemon.txt")
wiktionary <- read_terms("wiktionary.txt")
# Linear Search Function
linear_search <- function(query, search_values) {
# Input query value and vector
assert_that(is.vector(search_values))
sorted_data <- sort_input(search_values)
pattern <- paste("^", query, sep = "")
n <- length(search_values)
match_found <- vector(length = n)
for (i in 1:n) {
match_found[i] <- length(grep(pattern, sorted_data[i]))
}
indices_found <- which(match_found > 0)
# Check that it is sorted in increasing order
#assert_that(!is.unsorted(indices_found))
return(c(indices_found[1], indices_found[length(indices_found)]))
}
# Chooses random samples of n rows from provided files
random_sample_files <- function(nrows_of_sample) {
# Randomly choose one of the four test files
files <- c('movies.txt', "baby-names.txt", "pokemon.txt", "wiktionary.txt")
sample_file <- sample(files, 1)
file <- read_terms(sample_file)
# Random sample of n terms
sample_df <- sample_n(file, nrows_of_sample)
# Choose prefix randomly from list of common prefixes
prefix_list <- c('anti', 'de', 'dis', 'en', 'em', 'fore', 'in', 'im',
'inter', 'ir', 'ill', 'mis', 'mid', 'non', 'over', 'pre',
're', 'the', 'semi', 'sub', 'super', 'the', 'un', 'under')
rand_prefix <- sample(prefix_list, 1)
return(list(search_values = sample_df, query = rand_prefix))
}
# Generates random character strings
random_character_strings <- function(length_of_sample) {
# generates `length_of_sample` random terms and 1 random prefix
terms <- vector(length = length_of_sample)
for (i in 1:length_of_sample) {
# Generates term by randomly sampling letters
number_of_letters <- sample(1:10, 1)
sample_of_letters <- sample(letters, number_of_letters)
sample_term <- paste(sample_of_letters, collapse="")
terms[i] <- sample_term
}
# Random 3-letter prefix
sample_prefix <- sample(letters, 3)
prefix <- paste(sample_prefix, collapse="")
return(list(search_values = terms, query = prefix))
}
# Compares Output from Binary search and Linear Search with Random Input
tests_randomSamples <- function(randomizer_function, length_of_sample) {
# Takes input from either of the 2 randomizer function to generate random data
# Sample has n terms
# 100 simulations
matching_indices <- vector(length = 100)
for (i in 1:100) {
search_values <- randomizer_function(length_of_sample)$search_values
query <- randomizer_function(length_of_sample)$query
# Sort
if (is.data.frame(search_values)) {
search_values <- sort(search_values$term)
} else if (is.vector(search_values)) {
search_values <- sort(search_values)
}
assert_that(is.vector(search_values))
bs_indices <- occurrences(query, search_values)
ls_indices <- linear_search(query, search_values)
# Returns TRUE if indices match
match <- all((bs_indices == ls_indices)  |  (is.na(bs_indices) & is.na(ls_indices)))
matching_indices[i] <- match
}
return(all(matching_indices == TRUE))
}
source('autocomplete.R')
source('randomizedSamples.R')
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(microbenchmark))
# How performances Scales With Number of Words
# Uses random samples from movies file
random_sample_sizes <- function(n_times = 5) {
# Evaluate expression using microbenchmark n_times (default is 5)
file <- movies
sample_sizes <- numeric(69)
for (i in 1:69) {
values <- c(1:6, seq(6, 12.3, 0.1)[-1])
sample_sizes[i] <- exp(values[i])
}
time_at_samplesize <- vector(length = length(sample_sizes))
# Mean run time over n_times evaluations
for (i in 1:length(sample_sizes)) {
sample_of_movies <- sample_n(file, sample_sizes[i])
query <- 'the'
sorted_movies <- sort_input(sample_of_movies$term)
time_at_samplesize[i] <-
mean(microbenchmark(occurrences(query, sorted_movies), times = n_times)$time)
}
# Convert nanoseconds to Seconds
time_at_samplesize_seconds <- time_at_samplesize * 1e-9
fit <- lm(time_at_samplesize_seconds ~ log(sample_sizes))
options(device = "png")
png("performance.png")
plot(sample_sizes, time_at_samplesize_seconds, xlab = "Number of Words", ylab = "Time in Seconds",
main = "Performance With Number of Words")
curve(coef(fit)[1] + coef(fit)[2] * log(x), add = TRUE, col = 'red')
dev.off()
return(c(coef(fit)[1], coef(fit)[2]))
}
random_sample_sizes
random_sample_sizes()
profvis({
sorted_movies <- sort_input(movies$term)
occurrences("un", sorted_movies)
occurrences("the", sorted_movies)
})
library(profvis)
profvis({
sorted_movies <- sort_input(movies$term)
occurrences("un", sorted_movies)
occurrences("the", sorted_movies)
})
log2(200000)
log(200000)
6.287278e-05  * log(200000)
